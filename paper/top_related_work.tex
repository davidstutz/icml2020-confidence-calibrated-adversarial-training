\section{Related Work}
\label{sec:related-work}

\textbf{Adversarial Examples:}
%
Adversarial examples can roughly be divided into white-box attacks, \ie, with access to the model gradients, \eg \cite{GoodfellowICLR2015,MadryICLR2018,CarliniSP2017}, and black-box attacks, \ie, only with access to the model's output, \eg \cite{IlyasICML2018,NarodytskaCVPRWORK2017,AndriushchenkoARXIV2019}. Adversarial examples were also found to be transferable between models \cite{LiuICLR2017,XieCVPR2019}. In addition to \emph{imperceptible} adversarial examples, adversarial transformations, \eg,  \cite{EngstromICML2019,AlaifariICLR2019}, or adversarial patches \cite{BrownARXIV2017} have also been studied. Recently, projected gradient ascent to maximize the cross-entropy loss or surrogate objectives, \eg, \cite{MadryICLR2018,DongCVPR2018,CarliniSP2017}, has become standard. Instead, we directly maximize the confidence in any but the true class, similar to \cite{HeinCVPR2019,GoodfellowOPENREVIEW2019}, in order to effectively train and attack \ConfTrain.

\textbf{Adversarial Training:}
%
Numerous defenses have been proposed, of which several were shown to be ineffective \cite{AthalyeICML2018b,AthalyeARXIV2018b}. Currently, adversarial training is standard to obtain robust models. While it was proposed in different variants  \cite{SzegedyICLR2014,MiyatoICLR2016,HuangARXIV2015}, the formulation by \cite{MadryICLR2018} received considerable attention and has been extended in various ways: \cite{ShafahiAAAI2020,PielotARXIV2018} train on universal adversarial examples, in \cite{CaiIJCAI2018}, curriculum learning is used, and in \cite{TramerICLR2018,GrefenstetteARXIV2018} ensemble adversarial training is proposed. 
The increased sample complexity \cite{SchmidtNIPS2018} was addressed in \cite{LambAISEC2019,CarmonNIPS2019,UesatoNIPS2019} by training on interpolated or unlabeled examples. Adversarial training on multiple threat models is also possible \cite{TramerNIPS2019,MainiICML2020}. Finally, the observed robustness-accuracy trade-off has been discussed in \cite{TsiprasICLR2019,StutzCVPR2019,ZhangICML2019,RaghunathanARXIV2019}. Adversarial training has also been combined with self-supervised training \cite{HendrycksNIPS2019}. In contrast to adversarial training, \ConfTrain imposes a target distribution which tends towards a uniform distribution for large perturbations, allowing the model
to extrapolate beyond the threat model used at training time. Similar to adversarial training with an additional ``abstain'' class \cite{LaidlawARXIV2019}, robustness is obtained by rejection. In our case, rejection is based on confidence thresholding.

\textbf{Detection:}
%
Instead of correctly classifying adversarial examples, several works \cite{GongARXIV2017,GrosseARXIV2017,FeinmanARXIV2017,LiaoCVPR2018,MaICLR2018,AmsalegWIFS2017,MetzenICLR2017,BhagojiARXIV2017,HendrycksICLR2017,LiICCV2017,LeeNIPS2018} try to detect adversarial examples. However, several detectors have been shown to be ineffective against adaptive attacks aware of the detection mechanism \cite{CarliniAISec2017}. Recently, the detection of adversarial examples by confidence, similar to our approach with \ConfTrain, has also been discussed \cite{PangNIPS2018}. Instead, \citet{GoodfellowOPENREVIEW2019} focus on evaluating confidence-based detection methods using adaptive, targeted attacks maximizing confidence. Our attack, although similar in spirit, is untargeted and hence suited for \ConfTrain.