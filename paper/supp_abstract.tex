\icmltitle{Supplementary Material for\\Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks}

\vskip 0.3in

\begin{abstract}
    This document provides supplementary material for \textbf{confidence-calibrated adversarial training (\ConfTrain)}. First, in \secref{sec:supp-proof}, we provide the proof of Proposition 1, showing that there exist problems where standard adversarial training (\AdvTrain) is unable to reconcile robustness and accuracy, while \ConfTrain is able to obtain \emph{both} robustness \emph{and} accuracy. In \secref{sec:supp-experiments}, to promote reproducibility and emphasize our thorough evaluation, we discuss details regarding the used attacks, training procedure, baselines and evaluation metrics. Furthermore, \secref{sec:supp-experiments} includes additional experimental results in support of the observations in the main paper. For example, we present results for confidence threshold at $95\%$ and $98\%$ true positive rate (TPR), results for the evaluated detection baselines as well as per-attack and per-corruption results for in-depth analysis. We also include qualitative results highlighting how \ConfTrain obtains robustness through confidence thresholding. Code and pre-trained models are available at \href{http://davidstutz.de/ccat}{davidstutz.de/ccat}.
\end{abstract}

\section{Introduction}

\textbf{Confidence-calibrated adversarial training (\ConfTrain)} biases the network towards low-confidence predictions on adversarial examples. This is achieved by training the network to predict a uniform distribution between (correct) one-hot and uniform distribution which becomes more uniform as the distance to the attacked example increases. In the main paper, we show that \ConfTrain addresses two problems of standard adversarial training (\AdvTrain) as, \eg, proposed in \cite{MadryICLR2018}: the poor generalization of robustness to attacks not employed during training, \eg, other $L_p$ attacks or larger perturbations, and the reduced accuracy. We show that \ConfTrain, trained only on $L_\infty$ adversarial examples, improves robustness against previously unseen attacks through confidence thresholding, \ie, rejecting low-confidence (adversarial) examples. Furthermore, we demonstrate that \ConfTrain is able to improve accuracy compared to adversarial training. In this document, \secref{sec:supp-proof} provides the proof of Proposition 1. Then, \secref{sec:supp-experiments} includes details on our experimental setup, emphasizing our efforts to thoroughly evaluate \ConfTrain, and additional experimental results allowing an in-depth analysis of the robustness obtained through \ConfTrain.

In \secref{sec:supp-proof}, corresponding to the proof of Proposition 1, we show that there exist problems where standard adversarial training is indeed unable to reconcile robustness and accuracy. \ConfTrain, in contrast, is able to obtain \emph{both} robustness and accuracy, given that the ``transition'' between one-hot and uniform distribution used during training is chosen appropriately.

In \secref{sec:supp-experiments}, we discuss our thorough experimental setup to facilitate reproducibility and present additional experimental results in support of the conclusions of the main paper. In the first part of \secref{sec:supp-experiments}, starting with \secref{subsec:supp-experiments-attacks}, we provide a detailed description of the employed projected gradient descent (\PGD) attack with momentum and backtracking, including pseudo-code and used hyper-parameters. Similarly, we discuss details of the used black-box attacks. In \secref{subsec:supp-experiments-training}, we include details on our training procedure, especially for \ConfTrain. In \secref{subsec:supp-experiments-baselines}, we discuss the evaluated baselines, \ie, \cite{MainiICML2020,MadryICLR2018,ZhangICML2019,LeeNIPS2018,MaICLR2018}. Then, in \secref{subsec:supp-experiments-evaluation}, we discuss the employed evaluation metrics, focusing on our \emph{confidence-thresholded} robust test error (\RTE). In the second part of \secref{sec:supp-experiments}, starting with \secref{subsec:supp-experiments-ablation}, we perform ablation studies considering our attack and \ConfTrain. Regarding the attack, we demonstrate the importance of enough iterations, backtracking and appropriate initialization to successfully attack \ConfTrain. Regarding \ConfTrain, we consider various values for the hyper-parameter $\rho$ which controls the transition from one-hot to uniform distribution during training. In \secref{subsec:supp-experiments-analysis}, we analyze how \ConfTrain achieves robustness by considering its behavior in adversarial directions as well as in between clean examples. Finally, we provide additional experimental results in \secref{subsec:supp-experiments-results}: our main results, \ie, robustness against seen and unseen adversarial examples, for a confidence threshold at $95\%$ and $98\%$ true positive rate (TPR), per-attack results on all datasets and per-corruption results on MNIST-C \citep{MuICMLWORK2019} and Cifar10-C \citep{HendrycksARXIV2019}.