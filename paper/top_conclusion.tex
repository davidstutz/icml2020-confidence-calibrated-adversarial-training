\section{Conclusion}
\label{sec:conclusion}

Adversarial training results in robust models against the threat model \emph{seen} during training, \eg, $L_\infty$ adversarial examples. However, generalization to \emph{unseen} attacks such as other $L_p$ adversarial examples or larger $L_\infty$ perturbations is insufficient. We propose \textbf{confidence-calibrated adversarial training (\ConfTrain)} which biases the model towards low confidence predictions on adversarial examples and beyond. Then, adversarial examples can easily be rejected based on their confidence. Trained exclusively on $L_\infty$ adversarial examples, \ConfTrain improves robustness against unseen threat models such as larger $L_\infty$, $L_2$, $L_1$ and $L_0$ adversarial examples, adversarial frames, distal adversarial examples and corrupted examples. Additionally, accuracy is improved in comparison to adversarial training. We thoroughly evaluated \ConfTrain using $7$ different white-and black-box attacks with up to $50$ random restarts and $5000$ iterations. These attacks where adapted to \ConfTrain by directly maximizing confidence. We reported worst-case robust test error, extended to our confidence-thresholded setting, across \emph{all} attacks.